{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b419d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths\n",
    "# MODEL_OUTPUT_DIR = \"outputs/llama-3.2-11b-surgical-cholecT50---10frames_proc+task\"\n",
    "# BASE_MODEL_NAME = \"nvidia/Llama-3.2-11B-Vision-Surgical-CholecT50\"\n",
    "\n",
    "# MODEL_OUTPUT_DIR = \"outputs/llama-3.2-11b---10frames_proc+task\"\n",
    "MODEL_OUTPUT_DIR = \"outputs/llama-3.2-11b---10frames_proc+task_strict_instruction\"\n",
    "BASE_MODEL_NAME = \"nvidia/Llama-3.2-11B-Vision\"\n",
    "LORA_ADAPTERS_PATH = f\"{MODEL_OUTPUT_DIR}/lora_model\"\n",
    "VALIDATION_DATA_PATH = f\"outputs/llama-3.2-11b---10frames_proc+task_strict_instruction/val_ds\"\n",
    "\n",
    "# Inference configuration\n",
    "BATCH_SIZE = 4  # Adjust based on your GPU memory\n",
    "MAX_NEW_TOKENS = 100 # Max tokens to generate for each response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88af7e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.5: Fast Mllama patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.635 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a79fc2cef34ccb82bb0f33eb973222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MllamaForConditionalGeneration(\n",
       "  (model): MllamaModel(\n",
       "    (vision_model): MllamaVisionModel(\n",
       "      (patch_embedding): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), padding=valid, bias=False)\n",
       "      (gated_positional_embedding): MllamaPrecomputedPositionEmbedding(\n",
       "        (tile_embedding): Embedding(9, 8197120)\n",
       "      )\n",
       "      (pre_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "        (embedding): Embedding(9, 5120)\n",
       "      )\n",
       "      (post_tile_positional_embedding): MllamaPrecomputedAspectRatioEmbedding(\n",
       "        (embedding): Embedding(9, 5120)\n",
       "      )\n",
       "      (layernorm_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (layernorm_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): MllamaVisionEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MllamaVisionEncoderLayer(\n",
       "            (self_attn): MllamaVisionAttention(\n",
       "              (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "            )\n",
       "            (mlp): MllamaVisionMLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (global_transformer): MllamaVisionEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x MllamaVisionEncoderLayer(\n",
       "            (self_attn): MllamaVisionAttention(\n",
       "              (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)\n",
       "            )\n",
       "            (mlp): MllamaVisionMLP(\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)\n",
       "              (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): MllamaTextModel(\n",
       "      (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (3): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (8): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (13): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (18): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (23): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (28): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (33): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (38): MllamaCrossAttentionDecoderLayer(\n",
       "          (cross_attn): MllamaTextCrossAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "            (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "        (39): MllamaSelfAttentionDecoderLayer(\n",
       "          (self_attn): MllamaTextSelfAttention(\n",
       "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): MllamaTextMLP(\n",
       "            (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): MllamaRotaryEmbedding()\n",
       "    )\n",
       "    (multi_modal_projector): Linear4bit(in_features=7680, out_features=4096, bias=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = LORA_ADAPTERS_PATH, # The path to our saved LoRA adapters\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d87f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset loaded:\n",
      "Dataset({\n",
      "    features: ['answer', 'cvid', 'instrument', 'action', 'tissue', 'procedure', 'task', 'i', 'second', 'image', 'text'],\n",
      "    num_rows: 8420\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the validation dataset from disk\n",
    "val_ds = load_from_disk(VALIDATION_DATA_PATH)\n",
    "\n",
    "print(\"Validation dataset loaded:\")\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4129aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example inference prompt:\n",
      " <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Generate actionabl surgical feedback to a trainee surgeon in the context of robot-assisted urology surgery. Your output must be maximum 1 sentence. The surgical feedback must focus on the most teachable instrument-action-tissue event. The feedback must be concise and to the point. Do not generate any other information or context. Do not generate any additional text or explanation.<|image|>Procedure: radical prostatectomy\n",
      "Task: Endopelvic fascia<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the instruction and question templates used during training\n",
    "# instruction = 'Respond with 1-2 sentences of actionable feedback focused on the most teachable instrument-action-tissue event.'\n",
    "instruction = \"Generate actionabl surgical feedback to a trainee surgeon in the context of robot-assisted urology surgery. Your output must be maximum 1 sentence. The surgical feedback must focus on the most teachable instrument-action-tissue event. The feedback must be concise and to the point. Do not generate any other information or context. Do not generate any additional text or explanation.\"\n",
    "question_template = \"Procedure: {procedure}\\nTask: {task}\"\n",
    "\n",
    "def create_inference_prompt(example):\n",
    "    \"\"\"\n",
    "    Creates a prompt for inference by formatting the user's part of the conversation.\n",
    "    \"\"\"\n",
    "    question = question_template.format(procedure=example[\"procedure\"], task=example[\"task\"])\n",
    "    \n",
    "    # This structure must match the one used in `formatting_prompts_func` during training\n",
    "    text_data = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "            {\"type\": \"image\"}, \n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "        ]},\n",
    "    ]\n",
    "    \n",
    "    # Use add_generation_prompt=True for inference\n",
    "    prompt = tokenizer.apply_chat_template(text_data, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "print(\"Example inference prompt:\\n\", create_inference_prompt(val_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5358ff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(\n",
    "    cvids,\n",
    "    ground_truths,\n",
    "    predictions,\n",
    "    procedures,\n",
    "    tasks,\n",
    "    idxs,\n",
    "    seconds\n",
    "):\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'cvid': cvids,\n",
    "        'ground_truth': ground_truths,\n",
    "        'prediction': predictions,\n",
    "        'procedure': procedures,\n",
    "        'task': tasks,\n",
    "        'idx': idxs,\n",
    "        'seconds': seconds\n",
    "    })\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_csv_path = f\"{MODEL_OUTPUT_DIR}/inference_results.csv\"\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    print(f\"Inference complete. Results saved to {output_csv_path}\")\n",
    "\n",
    "    # Display the first 10 results for a quick check\n",
    "    print(\"\\nSample Inference Results:\")\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    display(results_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "cvids = []\n",
    "procedures = []\n",
    "tasks = []\n",
    "idxs = []\n",
    "seconds = []\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Disable parallelism to avoid warnings\n",
    "\n",
    "# Process the dataset in batches\n",
    "for i in tqdm(range(0, len(val_ds), BATCH_SIZE), desc=\"Running Inference\"):\n",
    "    batch_data = val_ds[i : i + BATCH_SIZE]\n",
    "    keys = batch_data.keys()\n",
    "    batch_data = [{key: batch_data[key][j] for key in keys} for j in range(len(batch_data['image']))]\n",
    "    \n",
    "    # 1. Prepare batch inputs\n",
    "    prompts = [create_inference_prompt(ex) for ex in batch_data]\n",
    "    images = [ex['image'] for ex in batch_data]\n",
    "\n",
    "    # Convert PIL images to numpy arrays if they are not already\n",
    "    images_nested = [[np.array(img)] for img in images]\n",
    "\n",
    "    # 2. Tokenize text and prepare inputs for the model\n",
    "    inputs = tokenizer(\n",
    "        text=prompts,\n",
    "        images=images_nested, # Pass the corrected nested list\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # 3. Generate text\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS, eos_token_id=tokenizer.eos_token_id, use_cache=False)\n",
    "    \n",
    "    # 4. Decode the generated text, skipping special tokens\n",
    "    decoded_outputs = tokenizer.batch_decode(generated_ids[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # 5. Store results\n",
    "    predictions.extend(decoded_outputs)\n",
    "    ground_truths.extend([ex['answer'] for ex in batch_data])\n",
    "    cvids.extend([ex['cvid'] for ex in batch_data])\n",
    "    procedures.extend([ex['procedure'] for ex in batch_data])\n",
    "    tasks.extend([ex['task'] for ex in batch_data])\n",
    "    idxs.extend([ex['i'] for ex in batch_data])\n",
    "    seconds.extend([ex['second'] for ex in batch_data])\n",
    "    \n",
    "    save_results(\n",
    "        cvids=cvids,\n",
    "        ground_truths=ground_truths,\n",
    "        predictions=predictions,\n",
    "        procedures=procedures,\n",
    "        tasks=tasks,\n",
    "        idxs=idxs,\n",
    "        seconds=seconds\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame({\n",
    "    'cvid': cvids,\n",
    "    'ground_truth': ground_truths,\n",
    "    'prediction': predictions,\n",
    "    'procedure': procedures,\n",
    "    'task': tasks,\n",
    "    'idx': idxs,\n",
    "    'seconds': seconds\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = f\"{MODEL_OUTPUT_DIR}/inference_results.csv\"\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Inference complete. Results saved to {output_csv_path}\")\n",
    "\n",
    "# Display the first 10 results for a quick check\n",
    "print(\"\\nSample Inference Results:\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(results_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surgfbgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
